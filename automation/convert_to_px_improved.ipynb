{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97df4f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Fire - Type of Incident.csv...\n",
      "\n",
      "Suggested Columns:\n",
      "Value Column: Value\n",
      "Stub Columns: ['DateCode']\n",
      "Heading Columns: ['FeatureCode', 'FeatureName', 'Accident Status', 'Type of Fire']\n",
      "\n",
      "Notes:\n",
      "- Suggested 'Value' as value_col (numeric, measurement-related).\n",
      "- Suggested 'DateCode' for stub_cols (time-related).\n",
      "- Suggested 'FeatureCode' for heading_cols (low cardinality: 3 unique values).\n",
      "- Suggested 'FeatureName' for heading_cols (low cardinality: 3 unique values).\n",
      "- Suggested 'Accident Status' for heading_cols (low cardinality: 3 unique values).\n",
      "- Suggested 'Type of Fire' for heading_cols (low cardinality: 8 unique values).\n",
      "\n",
      "Running conversion with the following configuration:\n",
      "input_file: Fire - Type of Incident.csv\n",
      "output_file: Fire - Type of Incident.px\n",
      "value_col: Value\n",
      "stub_cols: ['DateCode']\n",
      "heading_cols: ['FeatureCode', 'FeatureName', 'Accident Status', 'Type of Fire']\n",
      "title: Fire - Type of Incident\n",
      "subject_area: Incidents\n",
      "matrix_code: SCOSRHE\n",
      "decimals: 3\n",
      "source: Scottish Fire and Rescue Service\n",
      "agg_method: mean\n",
      "Loading data from Fire - Type of Incident.csv...\n",
      "Writing 130368 data points to Fire - Type of Incident.px...\n",
      "✅ PX file saved as: Fire - Type of Incident.px\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Fire - Type of Incident.px'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "\n",
    "def generate_matrix_code():\n",
    "    \"\"\"Generate a matrix code starting with 'SCO' followed by 4 random uppercase letters.\"\"\"\n",
    "    return \"SCO\" + ''.join(random.choices(string.ascii_uppercase, k=4))\n",
    "\n",
    "def px_escape(text):\n",
    "    \"\"\"Escape quotes in strings for PX format.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return text.replace('\"', '\"\"')\n",
    "    return text\n",
    "\n",
    "def suggest_columns(df):\n",
    "    \"\"\"\n",
    "    Suggest stub_cols, heading_cols, and value_col based on DataFrame content.\n",
    "    Returns a dictionary with suggestions.\n",
    "    \"\"\"\n",
    "    suggestions = {\n",
    "        'value_col': None,\n",
    "        'stub_cols': [],\n",
    "        'heading_cols': [],\n",
    "        'notes': []\n",
    "    }\n",
    "    \n",
    "    # Identify potential value column (numeric, likely a measurement)\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    for col in numeric_cols:\n",
    "        if df[col].notna().any():\n",
    "            # Prioritize columns with \"rating\", \"energy\", or similar terms\n",
    "            if any(keyword in col.lower() for keyword in ['rating', 'energy', 'score', 'value']):\n",
    "                suggestions['value_col'] = col\n",
    "                suggestions['notes'].append(f\"Suggested '{col}' as value_col (numeric, measurement-related).\")\n",
    "                break\n",
    "            # Fallback: numeric with decimals or moderate cardinality\n",
    "            elif df[col].dropna().apply(lambda x: x % 1 != 0).any() or (10 < df[col].nunique() < 500):\n",
    "                suggestions['value_col'] = col\n",
    "                suggestions['notes'].append(f\"Suggested '{col}' as value_col (numeric with decimals or moderate cardinality).\")\n",
    "                break\n",
    "    \n",
    "    if not suggestions['value_col']:\n",
    "        suggestions['notes'].append(\"No clear value_col found. Please specify manually.\")\n",
    "    \n",
    "    # Remaining columns for stub and heading\n",
    "    non_value_cols = [col for col in df.columns if col != suggestions['value_col']]\n",
    "    categorical_cols = df[non_value_cols].select_dtypes(include=['object', 'category']).columns\n",
    "    \n",
    "    # Suggest stub_cols (time, geographic, or moderate cardinality)\n",
    "    for col in categorical_cols:\n",
    "        unique_count = df[col].nunique()\n",
    "        if unique_count > 1:\n",
    "            if any(keyword in col.lower() for keyword in ['date', 'year', 'quarter', 'month']):\n",
    "                suggestions['stub_cols'].append(col)\n",
    "                suggestions['notes'].append(f\"Suggested '{col}' for stub_cols (time-related).\")\n",
    "            elif any(keyword in col.lower() for keyword in ['town', 'city', 'region', 'area', 'postcode']):\n",
    "                suggestions['stub_cols'].append(col)\n",
    "                suggestions['notes'].append(f\"Suggested '{col}' for stub_cols (geographic).\")\n",
    "            elif 20 <= unique_count <= 100:\n",
    "                suggestions['stub_cols'].append(col)\n",
    "                suggestions['notes'].append(f\"Suggested '{col}' for stub_cols (moderate cardinality: {unique_count} unique values).\")\n",
    "    \n",
    "    # Suggest heading_cols (low cardinality, e.g., ratings, types)\n",
    "    for col in categorical_cols:\n",
    "        if col not in suggestions['stub_cols']:\n",
    "            unique_count = df[col].nunique()\n",
    "            if 1 < unique_count <= 20:\n",
    "                suggestions['heading_cols'].append(col)\n",
    "                suggestions['notes'].append(f\"Suggested '{col}' for heading_cols (low cardinality: {unique_count} unique values).\")\n",
    "    \n",
    "    # Limit stub_cols to 3 to avoid excessive combinations\n",
    "    if len(suggestions['stub_cols']) > 3:\n",
    "        suggestions['stub_cols'] = suggestions['stub_cols'][:3]\n",
    "        suggestions['notes'].append(\"Limited stub_cols to first 3 to avoid excessive combinations.\")\n",
    "    \n",
    "    # Ensure at least one stub_col\n",
    "    if not suggestions['stub_cols'] and suggestions['heading_cols']:\n",
    "        suggestions['stub_cols'].append(suggestions['heading_cols'].pop(0))\n",
    "        suggestions['notes'].append(\"Moved one heading_col to stub_cols as no stub_cols were identified.\")\n",
    "    \n",
    "    return suggestions\n",
    "\n",
    "def tidy_to_pxstat(\n",
    "    input_file,\n",
    "    output_file=None,\n",
    "    stub_cols=None,\n",
    "    heading_cols=None,\n",
    "    title=None,\n",
    "    subject_area=None,\n",
    "    matrix_code=None,\n",
    "    decimals=None,\n",
    "    value_col=\"Value\",\n",
    "    source=\"Scottish Government\",\n",
    "    agg_method=\"mean\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert Tidy format CSV to monolingual PxStat format.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_file : str\n",
    "        Path to input CSV file.\n",
    "    output_file : str, optional\n",
    "        Path to output PX file (default: input_name + \".px\").\n",
    "    stub_cols : list, optional\n",
    "        Columns to use as stub dimensions (rows).\n",
    "    heading_cols : list, optional\n",
    "        Columns to use as heading dimensions (columns).\n",
    "    title : str, optional\n",
    "        Title for the PX dataset.\n",
    "    subject_area : str, optional\n",
    "        Subject area for the dataset.\n",
    "    matrix_code : str, optional\n",
    "        Matrix code for the dataset (default: generated as SCO+4 random letters).\n",
    "    decimals : int, optional\n",
    "        Number of decimals to use.\n",
    "    value_col : str, optional\n",
    "        Column name containing the values (default: \"Value\").\n",
    "    source : str, optional\n",
    "        Data source (default: \"Scottish Government\").\n",
    "    agg_method : str, optional\n",
    "        Aggregation method for duplicates (\"sum\" or \"mean\", default: \"mean\").\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {input_file}...\")\n",
    "    try:\n",
    "        # Load CSV with optimized dtypes\n",
    "        df = pd.read_csv(input_file, low_memory=False)\n",
    "        \n",
    "        # Convert categorical columns to 'category' dtype\n",
    "        for col in df.select_dtypes(include=['object']).columns:\n",
    "            df[col] = df[col].astype('category')\n",
    "        \n",
    "        # Validate required columns\n",
    "        if value_col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {value_col}\")\n",
    "        \n",
    "        # Use provided or default dimensions\n",
    "        if stub_cols is None or heading_cols is None:\n",
    "            suggestions = suggest_columns(df)\n",
    "            stub_cols = stub_cols or suggestions['stub_cols']\n",
    "            heading_cols = heading_cols or suggestions['heading_cols']\n",
    "            print(f\"Using stub_cols: {stub_cols}\")\n",
    "            print(f\"Using heading_cols: {heading_cols}\")\n",
    "        \n",
    "        # Validate dimensions\n",
    "        group_cols = stub_cols + heading_cols\n",
    "        if not group_cols:\n",
    "            raise ValueError(\"No valid dimension columns specified or detected.\")\n",
    "        for col in group_cols:\n",
    "            if col not in df.columns:\n",
    "                raise ValueError(f\"Dimension column '{col}' not found in data.\")\n",
    "        \n",
    "        # Simplify to needed columns\n",
    "        df_simple = df[group_cols + [value_col]].copy()\n",
    "        \n",
    "        # Make values numeric\n",
    "        df_simple[value_col] = pd.to_numeric(df_simple[value_col], errors=\"coerce\", downcast='float')\n",
    "        \n",
    "        # Fix duplicates by grouping with observed=True\n",
    "        df_simple = df_simple.groupby(group_cols, as_index=False, observed=True).agg({value_col: agg_method})\n",
    "        \n",
    "        # Use existing combinations (avoid full_index to prevent memory explosion)\n",
    "        indexed_df = df_simple.set_index(group_cols)\n",
    "        \n",
    "        # Generate data values\n",
    "        data_values = [\n",
    "            '\"..\"' if pd.isna(v) else str(round(v, decimals or 3))\n",
    "            for v in indexed_df[value_col]\n",
    "        ]\n",
    "        \n",
    "        # Metadata\n",
    "        creation_date = datetime.today().strftime(\"%Y%m%d %H:%M\")\n",
    "        if title is None:\n",
    "            title = f\"Data from {os.path.basename(input_file)}\"\n",
    "        if subject_area is None:\n",
    "            subject_area = \"Statistics\"\n",
    "        if matrix_code is None:\n",
    "            matrix_code = generate_matrix_code()\n",
    "        if decimals is None:\n",
    "            max_decimals = max(\n",
    "                [len(str(x).split('.')[-1]) if '.' in str(x) else 0 for x in df_simple[value_col].dropna()]\n",
    "            ) if not df_simple[value_col].dropna().empty else 0\n",
    "            decimals = min(max_decimals, 6)\n",
    "        \n",
    "        units = px_escape(df['Units'].iloc[0] if 'Units' in df.columns and not df['Units'].isna().all() else 'Count')\n",
    "        header = f\"\"\"CHARSET=\"UTF-16\";\n",
    "AXIS-VERSION=\"2013\";\n",
    "CREATION-DATE=\"{creation_date}\";\n",
    "MATRIX=\"{matrix_code}\";\n",
    "DECIMALS={decimals};\n",
    "SUBJECT-AREA=\"{px_escape(subject_area)}\";\n",
    "SUBJECT-CODE=\"{matrix_code[:4] if len(matrix_code) >= 4 else matrix_code}\";\n",
    "CONTENTS=\"{px_escape(title)}\";\n",
    "TITLE=\"{px_escape(title)} - by {', '.join(group_cols)}\";\n",
    "UNITS=\"{units}\";\n",
    "STUB=\"{','.join(f'\"{px_escape(col)}\"' for col in stub_cols)}\";\n",
    "HEADING=\"{','.join(f'\"{px_escape(col)}\"' for col in heading_cols)}\";\n",
    "SOURCE=\"{px_escape(source)}\";\n",
    "\"\"\"\n",
    "        \n",
    "        # VALUES and CODES blocks\n",
    "        dim_vals = {col: sorted(df_simple[col].dropna().unique()) for col in group_cols}\n",
    "        def px_values_and_codes(name, values):\n",
    "            quoted_vals = \",\".join(f'\"{px_escape(str(v))}\"' for v in values)\n",
    "            quoted_codes = \",\".join(f'\"{i+1:02d}\"' for i in range(len(values)))\n",
    "            return f'VALUES(\"{name}\")={quoted_vals};\\nCODES(\"{name}\")={quoted_codes};\\n'\n",
    "        \n",
    "        meta_parts = \"\".join(px_values_and_codes(col, dim_vals[col]) for col in group_cols)\n",
    "        \n",
    "        # Make output filename\n",
    "        if output_file is None:\n",
    "            output_file = os.path.splitext(input_file)[0] + \".px\"\n",
    "        \n",
    "        # Write to file\n",
    "        print(f\"Writing {len(data_values)} data points to {output_file}...\")\n",
    "        with open(output_file, \"w\", encoding=\"utf-16\") as f:\n",
    "            f.write(header)\n",
    "            f.write(meta_parts)\n",
    "            f.write(\"DATA=\\n\")\n",
    "            f.write(\" \".join(data_values) + \"\\n\")\n",
    "            f.write(\";\")\n",
    "        \n",
    "        print(f\"✅ PX file saved as: {output_file}\")\n",
    "        return output_file\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing file: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Step 1: Declare input file\n",
    "INPUT_FILE = \"Fire - Type of Incident.csv\"\n",
    "\n",
    "# Step 2: Analyze file and suggest columns\n",
    "print(f\"Analyzing {INPUT_FILE}...\")\n",
    "df_preview = pd.read_csv(INPUT_FILE, nrows=1000)  # Load a sample for analysis\n",
    "suggestions = suggest_columns(df_preview)\n",
    "\n",
    "print(\"\\nSuggested Columns:\")\n",
    "print(f\"Value Column: {suggestions['value_col']}\")\n",
    "print(f\"Stub Columns: {suggestions['stub_cols']}\")\n",
    "print(f\"Heading Columns: {suggestions['heading_cols']}\")\n",
    "print(\"\\nNotes:\")\n",
    "for note in suggestions['notes']:\n",
    "    print(f\"- {note}\")\n",
    "\n",
    "# Step 3: Configuration template (modify as needed)\n",
    "CONFIG = {\n",
    "    'input_file': INPUT_FILE,\n",
    "    'output_file': \"Fire - Type of Incident.px\",\n",
    "    'value_col': suggestions['value_col'],  # Override if needed\n",
    "    'stub_cols': suggestions['stub_cols'],  # Custom override\n",
    "    'heading_cols': suggestions['heading_cols'],  # Custom override\n",
    "    'title': \"Fire - Type of Incident\",\n",
    "    'subject_area': \"Incidents\",\n",
    "    'matrix_code': generate_matrix_code(),  # e.g., SCOABCD\n",
    "    'decimals': 3,\n",
    "    'source': \"Scottish Fire and Rescue Service\",\n",
    "    'agg_method': \"mean\"\n",
    "}\n",
    "\n",
    "# Step 4: Run the conversion\n",
    "print(\"\\nRunning conversion with the following configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "tidy_to_pxstat(**CONFIG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
